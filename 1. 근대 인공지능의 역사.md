# ê·¼ëŒ€ ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬
History of modern #Artificial_Inteligence
í˜„ëŒ€ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì´ ë‚˜íƒ€ë‚˜ê¸°ê¹Œì§€ ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì˜ ì—­ì‚¬ì—ì„œ ë‹¤ë¤„ì™”ë˜ ëª¨ë¸ë“¤ì„ ì£¼ì œì™€ ì‹œê°„ì— ë”°ë¼ ë‹¤ë¤„ë³´ì.


# ì£¼ì œë³„ ì—­ì‚¬
![[assets/Pasted image 20220911232132.png]]
- ê°™ì€ ì£¼ì œì˜ì‹ì„ ê³µìœ í•˜ëŠ” ì—°êµ¬ëŠ” ê°™ì€ ìƒ‰ í…Œë‘ë¦¬ë¡œ í‘œí˜„ë˜ì–´ ìˆë‹¤.
- ì²« ë²ˆì§¸ ê²¨ìš¸
	ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ë¶€ì¡±. ì‹ ê²½ë§ì— ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê¸° ì–´ë ¤ì› ë‹¤.
- ë‘ ë²ˆì§¸ ê²¨ìš¸
	í•˜ë“œì›¨ì–´ ê³„ì‚°ì†ë„ì˜ ë¶€ì¡±. í•„ìš”í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ì¶©ë¶„í•œ ì‹œê°„ ë‚´ì— í•˜ê¸° ì–´ë ¤ì› ë‹¤.
ì£¼ì œ: ì‹ ê²½ê³¼í•™ ë„ë©”ì¸ì—ì„œì˜ ì‹œê°


## ì‹ ê²½ëª¨ë¸
- MCP Model (McCulloch & Pitts, 1943)
- [[13. í¼ì…‰íŠ¸ë¡ (Perceptron)ê³¼ MLP; Multi-Layer Perceptron]] (Frank Rosenblatt, 1958)


## Plasticity
- Hebbian learning (Donald Hebb, 1949)
- Oja's rule (Erkki Oja, 1984)


## ê°•í™”í•™ìŠµ: action
[[ê°•í™”í•™ìŠµ ì´ë¡ ; Reinforcement Learning an Introduction ìš”ì•½|ê°•í™”í•™ìŠµ]]ì„ ì´ìš©í•œ ì ‘ê·¼ë°©ì‹
- Dynamic programming (Richard E. Bellman, 1957)
	- Bellman equation
		- ì°¨ì›ì˜ ì €ì£¼
- Actor-critic algorithm (Richard S. Sutton, 1983)
	- TD learning (1984)
- Q learning (Chris Watkins, 1992)


## ìƒì„±ëª¨ë¸: internal representation
- Back propagation (Paul Werbos, 1974)
- Back propagation, Autoencoder (David Rumelhart, Geoffrey Hinton, 1986)
- Helmholtz machine (Peter Dayan, 1998)
	- Q-Learning (1993)


## ì‹œê°ì²´ê³„
- Neocognitron (Kunihiko Fukushima, 1979)
- Convolutional Neural Network (Yann LeCun, 1989)


## ë¹„-ìˆœì „íŒŒ ì‹ ê²½ë§: cognition 
Feed forward(ìˆœì „íŒŒ)í•˜ì§€ ì•ŠëŠ” ì‹ ê²½ë§
- Hopfield network (John Hopfield, 1982)
- Boltzmann machine (Geoffrey Hinton, 1983)
- Deep Belief Network (Geoffrey Hinton, 2005)


## ìˆœì°¨ì  ë°ì´í„°: temporal domain
Sequential Dataì— ëŒ€í•´ ë‹¤ë£¨ëŠ” ì¸ê³µì§€ëŠ¥ ë¶„ì•¼
- Recurrent neural network (Michael I, Jodan, 1986)
- Long short-term memory (JÃ¼rgen Schmidhuber, 1997)


# ì‹œê°„ìˆœ ì—­ì‚¬
![[assets/Pasted image 20220911232132.png]]
- ì‹œê°„ ìˆœì„œë¡œ ì£¼ìš” ì—°êµ¬ë¥¼ ì•„ë˜ ì •ë¦¬í•˜ì˜€ë‹¤.


## ê·¹ì´ˆê¸°
- Initial concept of memory (John Locke, 1632-1704) (David Hartley, 1705-1757)
- Neural groupings (Alexander Bain, 1873)
- Markov process (Andrey Markov, 1906)
- Turing Machine, Turing test (Alan Turing, 1936)


## 1943 | MCP Model
(McCulloch & Pitts, 1943) ì‹ ê²½ ëª¨ë¸ì„ ì „ìì ìœ¼ë¡œ êµ¬í˜„.
[[13. í¼ì…‰íŠ¸ë¡ (Perceptron)ê³¼ MLP; Multi-Layer Perceptron]]


## 1949 | Hebbian learning
(Donald Hebb, 1949) Learning algorithmì„ ì œì‹œí•˜ê³ , ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚´.
[[ğŸ˜€ì¸ê³µì‹ ê²½ë§ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜; Hebbian learningë¶€í„° ì—­ì „íŒŒ(backpropagation)ê¹Œì§€]]


## 1958 | Perceptronì˜ ë“±ì¥
(Frank Rosenblatt, 1958) [[13. í¼ì…‰íŠ¸ë¡ (Perceptron)ê³¼ MLP; Multi-Layer Perceptron|í¼ì…‰íŠ¸ë¡ ]] í•­ëª©ì—ì„œ ë˜í•œ ìì„¸í•˜ê²Œ ì„¤ëª…. ì—­ì‚¬ì  ì˜ì˜ ì¤‘ í•˜ë‚˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ê³¼ ë‹¬ë¦¬ Sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ step function ë³´ë‹¤ ë‚˜ì€ Nonlinearityë¥¼ ì¶”ê°€í–ˆë‹¤ëŠ” ì ì´ë‹¤.


## 1962 | Genetic algorithm
(John Holland, 1962) ìœ ì „ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì  ì „ëµì„ ì°¾ì•„ë‚´ëŠ” ì—°êµ¬. ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì ‘ê·¼ì´ë‹¤.
Genetic algorithm


## 1968 | Group method of data handling
(Alexey Ivakhnenko, 1968) Deep learningê³¼ ìœ ì‚¬í•œ ìµœì´ˆì˜ ì‹œë„.
$$
Y(x_{1}, \dots , x_{n}) = a_{0} + \sum\limits_{i=1}^{m}a_{i}f_{i}
$$
$$
\begin{align}
Y\left(x_1, \ldots, x_n\right)=
&\begin{aligned}
a_{0}
\end{aligned} \\
&\begin{aligned}
+\sum_{i=1}^{n} a_i x_i
\end{aligned} \\
&\begin{aligned}
+\sum_{i=1}^n \sum_{j=i}^n a_{i j} x_i x_j
\end{aligned} \\
&\begin{aligned}
+\sum_{i=1}^n \sum_{j=i}^n \sum_{k=j}^n a_{i j k} x_i x_j x_k+\cdots
\end{aligned}
\end{align}
$$
- Output $Y$ë¥¼ elementary function $f_{i}$ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ í‘œí˜„
- Elementary functionì€ input variableì˜ ëª¨ë“  ê°€ëŠ¥í•œ ì„ í˜•ê³±ì˜ ê°€ì¤‘í•©


## ì²« ë²ˆì§¸ ê²¨ìš¸
ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” backpropagationì´ ì—°êµ¬ë˜ê¸° ì „ê¹Œì§€, í•™ìŠµ(ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸) ì•Œê³ ë¦¬ì¦˜ì˜ ë¶€ì¬ë¡œ ì¸í•œ ì²« ë²ˆì§¸ ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì˜ ê²¨ìš¸ì´ ìˆì—ˆë‹¤.


## 1974 | Backpropagation
(Paul Werbos, 1974) Back propagation í•­ëª©ì—ì„œ ì„¤ëª…. WerbosëŠ” ë°•ì‚¬í•™ìœ„ë…¼ë¬¸ì—ì„œ dynamic feedback ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ BP ê°œë…ì„ ì œì‹œí•œ ë°” ìˆë‹¤. Geoffrey Hintonì´ BP ê°œë…ì„ ë§Œë“  ê²ƒìœ¼ë¡œ ë„ë¦¬ ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì— [ë…¼ìŸ](https://www.reddit.com/r/MachineLearning/comments/g5ali0/d_schmidhuber_critique_of_honda_prize_for_dr/)ì´ ìˆë‹¤. 


## 1979| Neocognitron
(Fukushima, 1979) ì‹ ê²½ê³¼í•™ ê´€ì ì—ì„œëŠ” ì˜ì¥ë¥˜ ì‹œê°ì²´ê³„ ì´ˆê¸° ëª¨ë¸. í›„ì— Convolutional Neural Networkê°€ LeCunì— ì˜í•´ì„œ ì œì‹œë  ë•Œ, Fukushimaì˜ ì—°êµ¬ë¥¼ ì•Œê³  ìˆì§€ëŠ” ì•Šì•˜ë‹¤. í•˜ì§€ë§Œ MLPë¥¼ ì´ìš©í•œ feature extractionì˜ ê°œë…ì€ ì´ ë•Œ ì´ë¯¸ ì œì‹œë˜ì–´ ìˆì—ˆë‹¤.


## 1982 | Hopfield network
(John Hopfield, 1982)
[[2. ìˆœì „íŒŒí•˜ì§€ ì•ŠëŠ” ì‹ ê²½ë§]] í¬ìŠ¤íŒ…ì—ì„œ ë” ìì„¸í•œ ë‚´ìš©ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.


## 1983 | Boltzmann machine
(Geoffrey Hinton, 1983) Hopfield networkì™€ ê°™ì€ Energy minimization ê´€ì ì„ ê³µìœ í•¨.
Boltzmann machine


## 1983 | Actor-critic algorithm
(Richard S. Sutton, 1983) ê°•í™”í•™ìŠµì˜ ë³¸ê²©ì ì¸ ì‹œì‘. ê°•í™”í•™ìŠµì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ìˆ˜ì‹ê³¼ í”„ë¡œì„¸ìŠ¤ë‹¤. ë‹¤ìŒì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ rewardëŠ” environmentì™€ return ì‹œì ì— ì˜ì¡´í•œë‹¤(=ë‚´ì¬ì  uncertainty). ì–¸ì œ rewardê°€ ì£¼ì–´ì§ˆì§€, rewardë¥¼ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ policyë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤ëŠ” ê²ƒì€ ì–´ë–¤ ì˜ë¯¸ì¸ì§€ë¥¼ ì•Œê¸° ì–´ë µê¸° ë•Œë¬¸ì— RLì„ ì´ìš©í•´ í˜„ì‹¤ì˜ ë¬¸ì œë¥¼ í’€ê¸° ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
 1. Cycle:
	 1. State
	 2. Action
	 3. Reward (Return, Value, Q-functionìœ¼ë¡œ ê°œë…ì´ ê³ ë„í™”ë¨)
		 1. Policy update


## 1984 | ê°•í™”í•™ìŠµ: TD learning
Bellman Equation
![[assets/Pasted image 20220908002944.png]]
- Neuro Science ë¶„ì•¼ì—ì„œ rewardì— ëŒ€í•´ì„œ ë‹¤ë£°ë•Œ TD-errorì— ëŒ€í•œ ê°œë…ì„ ë§ì´ ë‹¤ë£¬ë‹¤.
- ì´í›„ ì €ì„œ: Reinforcement Learning. An introduction
- Q-learning
- Optimal Control Theory


## 1986 | Auto Encoder
(David Rumelhart, 1986)
- Internal representation(â‰ˆ encoding)

**AutoEncoderëŠ” Generative ëª¨ë¸ì¸ê°€?**
Deterministic encoderê¸° ë•Œë¬¸ì—, ì˜¤í† ì¸ì½”ë”ëŠ” ë‹¹ì—°íˆ inputê³¼ ìœ ì‚¬í•œ outputì„ ìƒì„±í•œë‹¤. ë•Œë¬¸ì— (ì‚¬ëŒì´ ì¸ì‹í•˜ê¸°ì—) ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” generative ëª¨ë¸ì´ë¼ ë¶ˆë¦´ ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì„ ë‹¬ì„±í•˜ì§€ëŠ” ëª»í–ˆë‹¤. í•˜ì§€ë§Œ í›„ì— Deep belief network(Hinton G, 2006)ì„ ë³´ë©´ DBNì„ í™•ë¥ ë¡ ì  autoencoder ë¡œì¨ ë¬˜ì‚¬í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ generative ëª¨ë¸ì˜ ì´ˆê¸° ì‹œë„ë¡œ ë³¼ ìˆ˜ ìˆê² ë‹¤. Generative model ë¶„ì•¼ì—ì„œëŠ” í›„ì— variational autoencoder ë˜í•œ ë“±ì¥í•˜ëŠ”ë°, ì´ ëª¨ë¸ ë˜í•œ í™•ë¥ ë¶„í¬ë¥¼ ì´ìš©í•´ ìƒì„±ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  ìˆë‹¤.
ê·¸ë ‡ì§€ë§Œ, DBNë¥˜ì˜ í™•ë¥ ë¶„í¬ë¥¼ ì´ìš©í•˜ëŠ” ìƒì„±ëª¨ë¸ì´ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆì§€ ëª»í•œë°, í•™ìŠµì´ ì˜ ì•ˆ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. í•™ìŠµì´ ì˜ ì•ˆë˜ëŠ” ì´ìœ ëŠ” í™•ë¥ ë¡ ì  ëª¨ë¸ë¡œì¨ sampling layerê°€ í•„ìˆ˜ì ì¸ë°, ì´ sampling ê³¼ì •ì´ computationally heavyí•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì´ë‹¤.

**ì‹ ê²½ê³¼í•™ ê´€ì ì—ì„œ ==Helmhotlz machine==ê³¼ì˜ ë¹„êµ**
Autoencoderë¥¼ ì ˆë°˜ìœ¼ë¡œ ì ‘ì–´ë†“ì€ ê²ƒê³¼ ê°™ì€ í˜•íƒœ. encoderì™€ decoderê°€ ê°™ì€ weightë¥¼ ê³µìœ í•œë‹¤ê³  ë³¼ ìˆ˜ë„ ìˆê² ë‹¤. í™ì„ì¤€ êµìˆ˜ë‹˜ê»˜ì„œ ë§ì”€í•˜ì‹œê¸°ë¡œëŠ”, ê°ê°ì„ ì¸ì½”ë”©í•˜ê³ , ì˜ˆìƒì„ ë””ì½”ë”©í•˜ëŠ” í˜•íƒœë¡œ ë‡Œì— êµ¬í˜„ë˜ì–´ìˆì§€ ì•Šì„ê¹Œ ìƒê°í•˜ê²Œ ëœë‹¤ëŠ”ë°, ì™œëƒí•˜ë©´ ê°™ì€ weightë¥¼ ê³µìœ í•˜ëŠ” í˜•íƒœê°€ connection ê´€ì ì—ì„œëŠ” encoding ë°©í–¥ê³¼ decoding ë°©í–¥ ì–‘ìª½ ëª¨ë‘ë¡œ connectionì´ ìˆëŠ” í˜•íƒœê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŠ” ì‹ ê²½ê³¼í•™ ê´€ì ì—ì„œ predictive codingì´ë¼ ë¶ˆë¦¬ëŠ” ê°œë…ì´ë‹¤.


## 1986 | Recurrent neural network
(Michael I. Jordan, 1986)
Recurrent neural network


## 1988 | Probabilistic reasoningê³¼ Bayesian network
(Judea Pearl, 1988)


## 1989 | Convolutional neural network
(Yahn LeCun, 1989)
Convolutional Neural Network

**Neocognitronê³¼ì˜ ì°¨ì´ì **
Neocognitronì€ ì˜ ë™ì‘í•˜ëŠ” ì‹œê°ëª¨ë¸ì˜ ì‹œì´ˆë¡œì¨, WTA(Winner Takes it All)ì´ë¼ëŠ” ìì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ì˜€ë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ Neocognitronì€ unsupervised learningê³¼ ì¼ë©´ ìœ ì‚¬ì„± ìˆëŠ” í•™ìŠµì„ í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, ì˜ í•™ìŠµë˜ì§€ëŠ” ì•Šì•˜ë‹¤. ì´í›„ CNNì´ í•™ìŠµì„±ëŠ¥ê³¼ ì‚¬ìš©ì„±ëŠ¥ ëª¨ë‘ ì¢‹ì€ ëª¨ìŠµì„ ë³´ì—¬ì£¼ë©´ì„œ CNNì´ ì‹œê°ëª¨ë¸ì˜ ê¸°ì´ˆë¡œ ì—¬ê²¨ì§€ê³  ìˆë‹¤.


## ë‘ ë²ˆì§¸ ê²¨ìš¸
ì»´í“¨íŒ… íŒŒì›Œì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì‹ ê²½ë§ ì—°êµ¬ì˜ ê²¨ìš¸ ì‹œê¸° ë™ì•ˆ shallow levelì˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ë§ì´ ì—°êµ¬ë˜ì—ˆë‹¤.
- Q-learning (Peter Dayan, 1993)
- Support vector machine (Vladimir Vapnik, 1995)
- Extream gradient boosting
ì´í›„ DBNì´ ë‚˜ì˜¤ë©´ì„œ í˜„ëŒ€ì  NNëª¨ë¸ê³¼ ì—°êµ¬ë“¤ì´ ë“±ì¥í•˜ê²Œ ëœë‹¤.


## ì´í›„
[[14. í•™ìŠµì— ëŒ€í•œ ë‹¤ì„¯ ê°€ì§€ ê´€ì ]]


# References
- https://www.youtube.com/embed/RBx0s-uqEh0
- https://www.youtube.com/embed/OMbg87Xyf2U
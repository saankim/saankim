# 휴레인 기말고사

## 1. 생명체에서의 Boundary는 어떤 의미인지 서술하시오. 서술 시 아래 개념들 중 최소4개 이상 포함하여 설명하시오.
- 생명체의 기본 조건 (Code-script, Reproduction, Evolution, Metabolism)
- Negative entropy와 free energy
- Maxwell’s Demon
- Life as an open system
- Homeostasis and allostasis
- Essential variables and Internal state
- Autonomy and Adaptability

생명체와 생명체 바깥을 나누는 boundary는 entropy gradient로 정의할 수도 있다. 생명체는 그 기능들, 이를테면 code-script, reproduction and evolution을 위해 내부의 homeostasis와 allostasis를 유지할 필요가 있다. 생명체의 기본 조건들은 모두 질서있는 물질분포를 필요로 하며, 질서있는 물질분포는 낮은 엔트로피를 가진다. 생명체의 boundary에 Maxwell's demon이 있는 것과도 같다. 하지만 현실에는 Maxwell's demon이 없기 때문에, negative entropy를 만드는 데는 많은 free energy가 필요하다. 곧 생명체의 생명활동 유지를 위해서는 높은 자유에너지가 필요한 것이다. 따라서 생명체는 주변으로부터 free energy를 harvesting 하며 살아간다. 이 때문에 생명체는 인간이 관념적으로 생각하는 것 처럼 독립적 system이 아니라 open system이다.



## 2. Bayesian Brain과 Predictive coding에 대해 서술하시오. 아래 개념들을 포함하여 설명하시오.
- Bayesian equation의 각 term의 인지과학적 의미
- Prediction과 prediction error에 관련된 수학식
- Inference와 learning의 의미

Bayesian brain과 predictive coding은 모두 점진적으로 주어지는 정보를 수집해 점진적으로 정보체계를 개선하는 방법이다. 이는 Bayesian equation을 토대로 볼 때 수학적으로 분명해지는데, 내부적 정보체계는 posterior, 점진적으로 주어지는 정보는 prior를 동반한 사전사건으로 볼 수 있다. 따라서 정보체계는 주어진 사건을 바탕으로 다음의 어떤 사건이 발생할 사후확률을 계산하고 estimate 하는 모델이며, 그 생물학적 구현체를 bayesian brain과 predictive coding이라 부를 수 있겠다. prediction과 prediction error의 갭이 클수록 더 정보체계가 추측한 사후확률이 부정확하다는 의미가 되고, 곧 사전확률에 많은 update가 필요해진다. bayesian theorem 식에서 $P(S \mid O)$가 prediction이며 실제 관측된 $P(S \mid O)$와 prior와 likelihood를 통해 계산된 $P(S \mid O)$의 차가 prediction error다. 이 과정에서 현재의 정보체계를 통해 사후확률을 계산해 내는 과정을 inference, 관측된 사건을 바탕으로 내부의 정보체계를 업데이트 하는 과정을 learning이라 부른다.



## 3. Hopfield network와 Self-attention의 계산 기제는 각각 아래와 같은데, 그 의미를 서술하시오.
- Hopfield network: $\xi^{\text {new }}=X \operatorname{softmax}\left(\beta \boldsymbol{X}^T \xi\right)$
- Self attention: $\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$
HN에서 새 $\xi$ 텐서는 input 텐서 $X$ 에 과거의 $\xi$ 텐서와 현재의 input $X$텐서의 곱을 softmax 한 것을 곱한 값이다. 곧, $\xi$는 X와 스스로의 유사도에 따라 softmax 가중치를 가지는 $X$다.
SA에서 attention function은 Q와 K의 유사도에 따라 softmax된 가중치를 가지는 V다. 이때 self-attention을 하기 때문에 스스로의 유사도를 가중치로 쓰게 된다.
곧 두 계산기제는 모두 스스로의 유사도에 따라 softmax 가중치를 value에 주는 방법이다.


## 4-1. 아래 그림을 참고하여 4개의 꼭지점에 있는 알고리즘들의 공통점과 차이점을 설명하시오.
![[assets/Pasted image 20221204003229.png]]
네 알고리즘은 모두 RL 알고리즘이다. 특히 descrete action-state transition sequence를 가정하고 있다.
1. DP는 state transition probability를 알고 있음이 가정된다. 따라서 full backup으로 모든 state를 BFS하게 탐색한다.
2. MC는 state에 대해 sampling된 정보를 가정한다. 따라서 DP처럼 모든 state를 방문하는 대신, 임의의 state를 종료시점까지 방문한 뒤 reward의 총합을 학습힌다. 종료시점까지 방문하는 deep backup을 가진다.
3. TD는 sample backup 상황에서 종료시점을 가정하거나 deep backup으로 인한 computing문제를 피하기 위해 shallow backup만을 사용해 policy를 업데이트한다.
4. Exhaustive search는 모든 state와 action을 종료시점까지 방문하는 진정한 의미의 BFS 탐색법이다.


## 4-2 아래 식과 pseudocode를 기반으로 Q learning과 Dyna-Q을 설명하시오.

Q learning:
![[assets/Pasted image 20221204002956.png]]
Dyna-Q:
![[assets/Pasted image 20221204003025.png]]
Q learning은 estimation으로 estimation하는 과정이라 말할 수 있다. value function을 직접 학습하기 어렵기 때문에 Q라는 proxy term을 통해 학습해나간다. 이는 수식에서는 target policy와 behavior policy의 차에 learning rate $\alpha$를 곱해 $Q$를 업데이트 하는 과정으로 표현된다. 슈도코드에서는 임의의 Q를 가정한 뒤 주어진 반복 수 $n$동안 $Q$를 수식과 같은 과정으로 업데이트 하는 식으로 표현된다. 이때 dyna-Q를 묘사하는 슈도코드에서는 model에 $R$과 $S$를 주어 학습시키는 동시에 모델로부터 $R$과 $S$를 얻어 학습에 사용한다. 


## 5. 아래Active inference에서 Variational Free energy, Expected Free energy를 바라보는 4가지 관점에 대해 각 index에 해당하는 term을 쓰시오. Index3,4와 Index5,6이 함의 하는  바를 자세히 기술하시오.
![[assets/Pasted image 20221204003042.png]]
1, 2식은 entropy와 energy 관점에서 VFE를 기술한다.
3, 4식은 bayesian 관점에서 VFE를 기술한다. 3에서 S에 대해, approximated posterior인 q와 prior인 P의 분포가 유사할수록 KL divergence는 작아지고, model이 complex 해지면서 q가 P를 잘 표상하게 된다. 하지만 이는 S에 대한 overfitting 일 수 있다. 따라서 4에서 S에 따라 기대되는 O를 predictive accuracy항으로 추가해서 S와 O의 표상관계를 잘 유지할 수 있도록, 곧 모델을 gerneralize 하도록 한다.
5, 6식은 epistemic value와 pramatic value 사이의 균형 관점에서 EFE를 설명한다. 특히 5는 관측된 $\ln q$와 예측된 $\ln q$의 차가 클수록 EFE를 작게 만드는 항이다. 곧 epistemic search를 장려하는 항이다. 반면 6은 surprizal이 클수록 작아지는 항이다. 곧, pragmatic exploitation을 장려하는 항이다.
7, 8식은 epistemic search 과정의 risk와 pragmatic exploitation 과정의 ambiguity, 곧 5, 6 식에서 제시하는 가치의 반대 방향에서 EFE를 설명한다.

1. Energy
2. Entropy
3. Model complexity
4. Predictive accuracy
5. Expected impormation gain, Epistemic value
6. Pragmatic value
7. Risk
8. Ambiguity


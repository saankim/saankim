# Likelihood
가능도, 우도; 주어진 관측값과 가설의 조합이 그럴싸한 정도

확률분포 $p$가 주어졌을 때, 어떤 사건 $x$의 확률(확률)은 $p(x)$로 구할 수 있다. 그렇다면, 사건 $x$가 주어졌을 때, 이 사건을 설명할 수 있는 확률분포가 $P_{1}$일 가능성(가능도)은 얼마일까?

확률과 가능성이라는 비슷한 두 단어가 동시에 쓰여서 헷갈린다. 예시로 우리에게 익숙한 주사위에서 시작해보자.
- 6면체 주사위에서 1의 눈이 나올 확률은 $\frac{1}{6}$
- 12면체 주사위에서 1의 눈이 나올 확률은 $\frac{1}{12}$

그렇다면 주사위 눈이 1이 나왔을 때, 던졌던 주사위가
- 6면체일 확률은 $\frac{1}{6}$
- 12면체일 확률은 $\frac{1}{12}$
라고 말할 수 있다.


## 형식화
수식을 통해 위 예시를 형식화해보자.
- 기호
	- 예시에서 있었던 표현
	- 의미
- $p_{\text{six}}$
	- 6면체 주사위
	- 확률분포. 어떤 사건들(주사위의 눈)과 그 확률을 담고 있다.
- $p_{\text{six}}(x=1)$
	- 1의 눈이 나올 확률
	- 어떤 사건에 대한 확률
- $L(p_{\text{six}}; x=1)$
	- 1의 눈이 나왔을때, 던진 주사위가 6면체 주사위일 가능성
	- 가능도

> [!Question] 왜 가능도를 $p(x=1\mid p_{\text{six}})$로 적지 않고 새로운 기호를 만들었을까?
> 모든 사건들에 대한 확률의 총합은 항상 1 이다. $\sum_{x} p = 1$
> 그러나 $\sum L \neq 1$ 일 수 있기 때문에. 넓은 의미에서 $L$은 확률이 아니다.


## 확률과 가능도의 비교
확률과 가능도의 차이를 알았으니, 전체 개념을 비교해보자.
- 모수들 $\theta$
- 확률분포 $p$
- 가능도 $L$
- 관측값 $X$

하나의 모수집합 $\theta$ 가 하나의 확률분포 $p$를 결정할 때 다음과 같이 말할 수 있다.
확률분포는 사건의 확률을 구하는 함수. 곧 임의의 사건을 $[0,1]$로 매핑하는 함수.
$$
p: x \rightarrow p
$$
가능도함수는 사건에 대한 확률분포의 가능도를 구하는 함수.
$$
L: X, p_{𝜃a} \rightarrow L
$$
가능도값을 실제로 구하거나 사용할 때는, 가능도의 파라미터가 되는 확률분포함수 $p_{\theta}(x)$의 값을 사용하게 된다. 곧, 가능도는 사건을 x축으로 하고, 확률을 y축으로 하는 확률밀도함수에서 y값, 확률과 같은 값을 사용하게 된다. 따라서 실용적으로는 가능도를 곧 다음과 같이 구하겠다.
$$
L = p(x=\text{obserbed})
$$



## 베이지안 통계학에서
베이지안 통계학에서 likelihood는 사후사건이 있을 때, 사전확률분포의 가능도다. 이때는 더 좁은 의미로 쓰여, 사전사건에 대한 사후사건의 조건부 확률로 다른 확률과 같은 차원에 있다.

두 사건에 대한 베이지안 통계학에서는, 두 번째 사건(보통 사후확률로 찾고자 하는 사건)이 일어났을때 사전확률이 일어날 확률을 의미한다. 예를 들어, 병을 가지고 있다는 것과 양성판정의 예시에서는 병을 앓고 있는 사람이 양성판정을 받을 확률이다.


# Marginal likelihood
ML을 공부하다보면 marginal likelihood를 표현하는 항이나, 최적화하는 대상으로 삼는 내용을 많이 만날 수 있다. 여기서 이 표현까지 이해해보자.


## Marginalization
marginalized out, integrated out

마지널라이즈는 결국 수식에서 (인간이) 집중하고 싶은 부분에 맞춰 식을 정리했다는 뜻이다. 이때 관심있는 항 외의 다른 모든 항들을 적분하게 된다. 그래서 integrated out 표현이 수식과 더 직접 이어진다고 하겠다.
$$
p(x \mid y)=\int_Z p(x, z \mid y) d z
$$
위 식에서는 $p(x, z \mid y)$로 표현되는 $x, z$의 확률 $p$를 $x$와 $y$에 대한 식으로 만들기 위해 우리가 관심 없는 항인 $z$를 $\int  \, dz$를 이용해 제거하고 있다.


# Maximum Likelihood Estimator와 머신러닝
MLE, 최대 우도 추정

## 관측값이 많을 때 가능도함수의 정의와 log likelihood
확률변수 $X$와 같은 확률밀도함수와 분포를 따라 관측된 $x_1, \cdots, x_n$ 이 있다고 하자. 확률밀도함수는 $\theta$ 를 모수로 가지고 $p(\theta;x)$로 쓰자. 모든 관측값을 가장 잘 설명하는 확률밀도함수를 찾기 위한 우도함수를 어떻게 정의해야 할까? 가장 간단한 정의로 각각의 관측$x$에 대한 가능도$L$을 모두 곱한 값을 생각할 수 있다.
$$
L(\theta ; x):=\prod_{k=1}^n p\left(x_k ; \theta\right)
$$
우리의 주된 관심사는 $L$을 최대화하는 $\hat{\theta}$를 찾는 것이다.
$$
\hat{\theta}= \underset{\theta}{\arg \max } L(\theta ; \mathbf{X})
$$
$\arg \max$에 영향을 주지 않고, 양변에 $\log$를 씌워서 곱연산을 덧셈연산으로 만들어 계산을 편하게 할 수 있다.
$$
\log L(\theta ; \mathbf{x}):=\sum_{k=1}^n \log p\left(x_k ; \theta\right)
$$


## 머신러닝에서
주어진 데이터에 잘 맞는 모델을 만들고 싶어하는 ML 관점에서 살펴보자. ML에서 만들고자 하는 모델이 결국 $p_{\theta}()$ 함수라고 할때 데이터를 잘 설명하는 $\theta$ 가 필요하다. 가능도가 어떤 데이터를 주어진 확률분포로 설명할 수 있는 정도라고 할 때 결국 최적의 $\theta$를 찾는 문제와 이어진다. 곧, 머신러닝 문제에서 마지널 가능도를 최적화 한다는 의미는 $L$을 $\theta$에 대한 식으로 표현한 뒤 $L$을 최대화하는 모델의 파라미터 $\theta$를 찾는 것과 같다.

여기서부터는 $\hat{\theta}$를 구하기 위해 최적화 방법이 사용된다. 가장 간단한 예시는 $L(\theta)$가 미분가능할 때, 미분값이 $0$이 되는 지점을 찾는 것이다. 이를 편미분방정식으로 적고 estimating equation이라 부르기도 한다. $\frac{\partial l(\theta)}{\partial \theta}=0$


## Log marginal likelihood
특히 Gaussina random process에 대해 log marginal likelihood라는 표현이 자주 쓰인다. 이는 위에서 설명한 마지널가능도에 계산의 편의를 위해 log를 씌운 것이다.


# References
- https://m.blog.naver.com/sw4r/221380395720
- https://www.cs.ubc.ca/~nando/540b-2011/lectures/l9.pdf
- https://freshrimpsushi.github.io/categories/수리통계학/
- https://freshrimpsushi.github.io/posts/optimizer-argmax-and-argmin/
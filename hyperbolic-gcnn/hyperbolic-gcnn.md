# Hyperbolic GCNN
Hyperbolic Graph Convolutional Neural Networks
- NeurIPS 2019
- 302 Citations in Google Scholar
- ì €ì
  - Stanford University
  - cs224w ê°•ì˜ë¥¼ ì§„í–‰í•˜ëŠ” ë“± network ê´€ë ¨ ì—°êµ¬ì‹¤


## Overview
ìŒê³¡ë©´ì„ ì»¤ë„ë¡œ ì‚¬ìš©í•˜ëŠ” Graph neural network.

ë…¸ë“œë“¤ì˜ feature vectorë¥¼ ìŒê³¡ë©´ ìœ„ì— í”Œë¡¯í•œë‹¤. ìŒê³¡ë©´ê³¼ í•œ ì ì—ì„œ ì ‘í•˜ëŠ” í‰ë©´-í•˜ì´í¼ë³¼ë¦­íƒ„ì  íŠ¸ ê³µê°„-ìœ„ì— í”Œë¡¯ëœ feature ë²¡í„°ë“¤ì„ ì‚¬ì˜í•œë‹¤. ì´ë¥¼ í†µí•´ ê°€ê¹Œìš´ ë…¸ë“œë“¤ì˜ feature ë¶„í¬ë¥¼ í™•ëŒ€í•˜ê³  ë¨¼ ë…¸ë“œë“¤ì˜ feature ë¶„í¬ë¥¼ ì¶•ì†Œí•˜ëŠ” íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤. ê²Œë‹¤ê°€ í•˜ì´í¼ë³¼ë¦­íƒ„ì  íŠ¸ í‰ë©´ì€ í‰ë©´ì´ê¸° ë•Œë¬¸ì— feature aggregationì„ ìœ„í•œ algebraic operationë“¤ì´ ì •ì˜ë˜ê³  ìˆ˜í–‰ë  ìˆ˜ ìˆë‹¤. í•˜ì´í¼ë³¼ë¦­íƒ„ì  íŠ¸ ê³µê°„ì—ì„œ node feature aggregation ê²°ê³¼ë¡œ ìƒì„±ëœ node featureë¥¼ ë‹¤ì‹œ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì— ì‚¬ì˜í•œë‹¤. ì—¬ê¸°ì„œ propositionì„ í†µí•´ í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ ê³µê°„ê³¼ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì´ ì‘ì€ ë²”ìœ„ì—ì„œ ì •ë³´ì†ì‹¤ ì—†ì´ ì‚¬ì˜ë  ìˆ˜ ìˆìŒì„ í™•ì¸í•œë‹¤. ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ ìœ„ì—ì„œ featrue aggregationì„ ìˆ˜í–‰í•œ ê²ƒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ë‚´ê³ ì í•¨ì´ë‹¤. ì¶”ê°€ë¡œ, hyperbolic space â†”ï¸ hyperbolic tangent spaceì˜ ê³„ì‚°ì€ hyperbolic trigonometric functionì˜ ì¼ì¢…ì´ì, exporential & logaric functionìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¯¸ë¶„íŠ¹ì„±ì´ ì¢‹ë‹¤.

![Untitled](hyperbolic.png)
- $\mathbb{H}^{d, K}$
	ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì˜ ìŒê³¡ë©´. $d$ì°¨ì›ì˜ ìŒê³¡ë©´ ê³µê°„
- $\mathcal{T}_{\mathbf{x}} \mathbb{H}^{d, K}$
	í‰ë©´. $d$ì°¨ì›ì˜ í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ ê³µê°„
- $\textrm x_j^H$
	ì£¼í™© ì . feature vector of a node
- $0$
	ë¹¨ê°„ ì . ì™œê³¡ì´ ìµœì†Œê°€ ë˜ëŠ” íƒ„ì  íŠ¸ ê³µê°„ê³¼ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì˜ ì ‘ì 
- $\operatorname{AGG}^{K}(\mathbf{x}^{H})_{i}$
	ì´ˆë¡ ì . aggregationëœ feature vector
- $\log$ì™€ $\exp$
	hyperbolic space â†”ï¸ hyperbolic tangent space ê°„ì˜ ì „í™˜ì„ ìˆ˜í–‰í•˜ëŠ” ì»¤ë„ í•¨ìˆ˜


### Problem setting
ìœ í´ë¦¬ë””ì–¸ ê³µê°„ $\rm E$(â†”ï¸ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ $\rm H$)ì— ìˆëŠ” $d$ ì°¨ì›ì˜ ë…¸ë“œ íŠ¹ì„±ì´ $0$ ë²ˆì§¸ ë ˆì´ì–´ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ ë•Œ, ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$(\rm x^{0,E} \it \_i )_{i \in \mathcal{V}}$$
$$f:\left(\mathcal{V}, \mathcal{E},\left(\mathbf{x}_{i}^{0, E}\right)_{i \in \mathcal{V}} \right) \rightarrow Z \in \mathbb{R}^{|\mathcal{V}| \times d^{\prime}}$$

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Graph neural networkë¥¼ ê°œì„ ì‹œí‚¤ê¸° ìœ„í•´ ë” ë‚˜ì€ ì„ë² ë”©í•¨ìˆ˜ $f$ë¥¼ ë§Œë“ ë‹¤. ì„ë² ë”©í•¨ìˆ˜ëŠ” ë…¸ë“œ, ì—ì§€, ì´ì›ƒí•œ ë…¸ë“œì˜ feature vector $\left(\mathcal{V}, \mathcal{E},\left(\mathbf{x}_{i}^{0, E}\right)_{i \in \mathcal{V}} \right)$ë¥¼ feature vectorì˜ ì§‘í•© $Z$ë¡œ ë³€í™˜í•œë‹¤. ëª¨ë“  feature vectorê°€ ì´ì›ƒí•œ ë…¸ë“œì˜ ëª¨ë“  feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´, $Z$ì˜ ì°¨ì›ì€ feature vectoreì˜ ì°¨ì›ì´ $d$ì¼ë•Œ $|\mathcal{V}| \times d$ ì°¨ì›ì´ë‹¤. ì„ë² ë”© í•¨ìˆ˜ $f$ì˜ ëª©ì ì€ $|\mathcal{V}| \times d$ë³´ë‹¤ ì‘ì€ $|\mathcal{V}| \times d^{\prime}$ ì°¨ì›ì— feature vectorë¥¼ ì„ë² ë”©í•˜ëŠ” ê²ƒì´ë‹¤.


## Background

### GCNNê³¼ ë¹„êµ
Graph Convolutional Neural Networkì—ì„œëŠ” feature representation $h$ë¥¼ $h=W \rm x + b$ ê¼´ì˜ linear transformì˜ ì—°ì†ìœ¼ë¡œ ìƒì„±í•œë‹¤. ì´í›„ neighborhood aggregation(convolution analogy)ë¥¼ í†µí•´ feature vector $\mathbf{x}_i^{\ell, E}$ ë¥¼ ìƒì„±í•œë‹¤.

- Feature transform
$$
\mathbf{h}_{i}^{\ell, E}=W^{\ell} \mathbf{x}
_i^{\ell-1, E}+\mathbf{b}^{\ell}
$$
- Neighborhood aggregation
$$
\mathbf{x}_i^{\ell, E}=\sigma\left(\mathbf{h}_i^{\ell, E}+\sum_{j \in \mathcal{N}(i)} w_{i j} \mathbf{h}_{j}^{\ell, E}\right)
$$


### ê¸°í•˜ ê´€ì 
Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature $c$. Here, we work with the hyperboloid model for its simplicity and its numerical stability.
- ì›
	$x^2 + y^2 = r^2$
- ìŒê³¡ì„ 
	$-x^2+y^2 = c$


### Hyperbolic ì´ë©´
$$
\mathbb{H}^{d, K}:=\left\{\mathbf{x} \in \mathbb{R}^{d+1}:\langle\mathbf{x}, \mathbf{x}\rangle_{\mathcal{L}}=-K, x_{0}>0\right\} \quad
$$
$$
\mathcal{T}_{\mathbf{x}} \mathbb{H}^{d, K}:=\left\{\mathbf{v} \in \mathbb{R}^{d+1}:\langle\mathbf{v},\mathbf{x}\rangle_{\mathcal{L}}=0\right\}
$$
- $\mathbb{H}^{d, K}$ has negative curvature $-1/K$
    - Minkowski inner product
		$<\mathbf{x}, \mathbf{y}>_\mathcal{L} := -x_0y_0 + x_1y_1 + \dots + x_dy_d$
		dot productì˜ ë³€í˜•ìœ¼ë¡œ, ìŒì˜ ê³¡ë¥ ì„ ë§Œë“¤ê¸° ìœ„í•´ í•œ ê°€ì§€ ì¶• ë°©í–¥ìœ¼ë¡œ ìŒì˜ dot product í•­ì´ ìˆë‹¤.
- Euclidean tangent space at point $\rm x$
	- tangent space â‰¡ set of orthogonal vectors
- $\rm v, w$ëŠ” ì•„ë˜ ì‹ì„ ë§Œì¡±í•˜ëŠ” ë¦¬ë§Œ ë©”íŠ¸ë¦­ í…ì„œ $\in$  ë¦¬ë§Œ ê³¡ë©´
$$
\mathcal{T}_{\mathbf{x}} \mathbb{H}^{d, K}, g_{\mathbf{x}}^{K}(\mathbf{v}, 
\mathbf{w}):=\langle\mathbf{v}, 
\mathbf{w}\rangle_{\mathcal{L}}
$$

- ë¦¬ë§Œê³¡ë©´ì€ êµ­ì†Œì ìœ¼ë¡œ ë³µì†Œí‰ë©´ê³¼ ë™í˜•
	- ë”°ë¼ì„œ êµ­ì†Œì ìœ¼ë¡œëŠ” Euclidean ì—°ì‚° ê°€ëŠ¥
		â‡’ ì „ì²´ ê³µê°„ì—ì„œëŠ” ìœ í´ë¦¬ë“œ ì—°ì‚°ì´ ì •ì˜ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìœ ìš©


### Geodesics and induced distances
![Untitled](curvature.png)
- ì²« ë²ˆì§¸ ê·¸ë¦¼
	Tangent space ë¥¼ 2ì°¨ì›ì— ì‚¬ì˜í•˜ë©´ PoincarÃ© diskì™€ ê°™ë‹¤
- ë‘ ë²ˆì§¸ ê·¸ë¦¼
	ê³¡ë¥ ì´ ì»¤ì§€ë©´ ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ëŠ˜ì–´ë‚œë‹¤
- ì„¸ ë²ˆì§¸ ê·¸ë¦¼
    - íšŒìƒ‰ ì„  hyperbolic parallel linesâ€¦

ì´ ê·¸ë¦¼ê³¼ ìœ í´ë¦¬ë“œ í‰ë©´ì„ ìƒê°í•˜ë©´ì„œ,
- Geodesics and distance functions are particularly important in graph embedding algorithms, as a common optimization objective is to minimize geodesic distances between connected nodes.


### Proposition
![Untitled](propo3.1.png)
- í•˜ì´í¼ë³¼ë¦­ ê³µê°„ Hìœ„ì˜ í•œ ì  x
- xë¥¼ í¬í•¨í•˜ëŠ” íƒ„ì  íŠ¸ ê³µê°„ TH ìœ„ì˜ í•œ ë²¡í„° uë¥¼ unit-speed  ë¼ê³  í•˜ì
- unique unit-speed ë¥¼ ê°€ì§€ëŠ” geodesic ğ›¾ ê°€ ìˆë‹¤
    - ğ›¾ëŠ” xê°€ ì›ì 
    - ğ›¾ëŠ” tì— ëŒ€í•œ ğ›¾ ìœ„ì˜ í•œ ë²¡í„°ì˜ ë¯¸ë¶„ì´ uë‹¤
- ì´ë•Œ ğ›¾ ëŠ” ì„ í˜• t ê³µê°„ì„ Kì— ì˜ì¡´í•˜ëŠ” cosh, sinhë¡œ ë³€í˜•í•œ ê³µê°„ì´ë‹¤
- ë”°ë¼ì„œ H ìœ„ì˜ ë‘ ì ì˜ ê±°ë¦¬ëŠ” Kì— ì˜ì¡´í•˜ëŠ” arccosh(TH norm)ì˜ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤
    - $\mathcal L$ì€ THìœ„ì˜ ë…¸ë¦„ì„ ì˜ë¯¸


### ì´ ìˆ˜ì‹ì˜ ì˜ë¯¸ëŠ”,
THë¥¼ ì˜ ì •ì˜í•˜ë©´, Hì˜ ê±°ë¦¬ë¥¼ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤ (íŠ¹íˆ, ë¯¸ë¶„ì´ ì‰¬ìš´ í˜•íƒœë¡œ)
	â‡’ ì¼ì¢…ì˜ ì»¤ë„


### Exporential and logarithmic maps
Hì™€ TH ì‚¬ì´ì˜ ë§¤í•‘ì„ ìœ„í•œ ì¶”ê°€ ì»¤ë„.
- $\rm exp^K_x(v) := \gamma(1)$ì„ ì´ìš©
- In general Riemannian manifolds, these operations are only defined locally but in the hyperbolic space, they form a bijection between the hyperbolic space and the tangent space at a point.
    - êµ¬ë©´ì—ì„œëŠ” ì›ì„ ëŒì•„ ì˜¤ë©´ ê³„ì† ê°™ì€ ì ì´ ë°˜ë³µë˜ì§€ë§Œ, ìŒê³¡ë©´ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šì€ ì„±ì§ˆì„ ì´ìš©

ê²°ê³¼ì ìœ¼ë¡œëŠ” ì•„ë˜ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
![Untitled](proposition.png)


## Method
1. Mapping from Euclidean to hyperbolic spaces
	Hyperbolic distance ì´ìš©
	$\mathbf{x}^{0, H}=\exp _{\mathbf{o}}^{K}\left(\left(0, \mathbf{x}^{0, E}\right)\right)=\left(\sqrt{K} \cosh \left(\frac{\left\|\mathbf{x}^{0, E}\right\|_{2}}{\sqrt{K}}\right), \sqrt{K} \sinh \left(\frac{\left\|\mathbf{x}^{0, E}\right\|_{2}}{\sqrt{K}}\right) \frac{\mathbf{x}^{0, E}}{\left\|\mathbf{x}^{0, E}\right\|_{2}}\right)$
2. Feature transform in hyperbolic space
    Hyperboloid linear transform ì´ìš©
	$W \otimes^{K} \mathbf{x}^{H}:=\exp_{\mathbf{o}}^{K}\left(W \log_{\mathbf{o}}^{K}\left(\mathbf{x}^{H}\right)\right)$
	$\mathbf{x}^{H} \oplus^{K} \mathbf{b}:=\exp_{\mathbf{x}^{\mathbf{H}}}^{K}\left(P_{\mathbf{o} \rightarrow \mathbf{x}^{H}}^{K}(\mathbf{b})\right)$
3. Neighborhood aggregation on the hyperboloid manifold  
	Attention based aggregation ì´ìš©
	$w_{i j}=\operatorname{SOFTMAX}_{j \in \mathcal{N}(i)}\left(\operatorname{MLP}\left(\log _{\mathbf{o}}^{K}\left(\mathbf{x}_{i}^{H}\right) \| \log _{\mathbf{o}}^{K}\left(\mathbf{x}_{j}^{H}\right)\right)\right)$
	$\operatorname{AGG}^{K}\left(\mathbf{x}^{H}\right)_{i}=\exp _{\mathbf{x}_{i}^{H}}^{K}\left(\sum_{j \in \mathcal{N}(i)} w_{i j} \log _{\mathbf{x}_{i}^{H}}^{K}\left(\mathbf{x}_{j}^{H}\right)\right)$
	- i, j ì— ëŒ€í•´
	- logë¡œ tangent spaceë¡œ ì˜®ê²¨ì„œ linear transformì„ ì •ë‹¹í™”í•˜ê³ 
	- || ë¡œ concat í•˜ê³ 
	- MLPë¡œ ì„ë² ë”©í•˜ëŠ” ê³¼ì •ì„
	- ëª¨ë“  ì´ì›ƒì— ëŒ€í•´ ìˆ˜í–‰í•˜ê³ ì„œ
	- SOFTMAX í•œë‹¤
	â‡’ ê²°ê³¼ë¡œ ë‚˜ì˜¨ wë¥¼
		- exp ì— ë„£ì–´ì„œ ë‹¤ì‹œ Hìœ„ë¡œ ì˜®ê¸´ë‹¤


### Non-linear activation with different curvatures
$$\sigma^{\otimes^{K_{\ell-1}, K_{\ell}}}\left(\mathbf{x}^{H}\right)=\exp _{\mathbf{o}}^{K_{\ell}}\left(\sigma\left(\log _{\mathbf{o}}^{K_{\ell-1}}\left(\mathbf{x}^{H}\right)\right)\right)$$
- $K_l$ ë¡œ ì ì ˆí•œ Kë¥¼ learnable
- ğœ ë¡œ non-linear
- log, exp ë¥¼ ê±°ì¹˜ë©° ì»¤ë„ë¡œ ê³µê°„ ë³€í™˜


## Experiments
ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì•„ë˜ ë„¤ ê°€ì§€ë¥¼ ë¹„êµí•˜ì˜€ìœ¼ë©°
- ë³´í†µ NN
- ì£¼ë³€ë§Œ ë³´ëŠ” GNN
- ë” í° ì£¼ë³€ì„ Euclidean í•˜ê²Œ ë³´ëŠ” GNN
- HGCN
	ìµœì  $\rm K$ ë¥¼ ì°¾ê¸° ìœ„í•œ íƒìƒ‰ë„ ì§„í–‰í•˜ì˜€ë‹¤.


## Results
community structure ê°€ ë¶„ëª…í•œ ê³³ì—ì„œ ê¸°ì¡´ë³´ë‹¤ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
![Untitled](result-table.png)

![Untitled](result-roc-auc.png)


## Conclusion
